{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "zeitanfang = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Exampledataframe.csv\", sep=';', header=1)\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "df = df.rename(columns={'answer_nps_comment': 'Review'})\n",
    "df = df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Quality of Work</th>\n",
       "      <th>Attitude &amp; Behaviour</th>\n",
       "      <th>Communication &amp; Information</th>\n",
       "      <th>Concept &amp; Handling</th>\n",
       "      <th>Quality of Product</th>\n",
       "      <th>Appointment &amp; Welcome</th>\n",
       "      <th>Mobility &amp; Transfer</th>\n",
       "      <th>Location &amp; Facility</th>\n",
       "      <th>Competence</th>\n",
       "      <th>Pricing &amp; Discount</th>\n",
       "      <th>Process</th>\n",
       "      <th>Drop Off &amp; Pick Up</th>\n",
       "      <th>Marketing &amp; Communication</th>\n",
       "      <th>Offer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anonymisierten Kundenrezension 1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anonymisierten Kundenrezension 2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymisierten Kundenrezension 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Review Quality of Work Attitude & Behaviour  \\\n",
       "1  Anonymisierten Kundenrezension 1               1                    1   \n",
       "2  Anonymisierten Kundenrezension 2               1                  NaN   \n",
       "3  Anonymisierten Kundenrezension 3             NaN                    1   \n",
       "\n",
       "  Communication & Information Concept & Handling Quality of Product  \\\n",
       "1                         NaN                NaN                  0   \n",
       "2                         NaN                NaN                NaN   \n",
       "3                           1                NaN                 -1   \n",
       "\n",
       "  Appointment & Welcome Mobility & Transfer Location & Facility Competence  \\\n",
       "1                   NaN                 NaN                 NaN          1   \n",
       "2                   NaN                 NaN                 NaN        NaN   \n",
       "3                   NaN                 NaN                 NaN        NaN   \n",
       "\n",
       "  Pricing & Discount Process Drop Off & Pick Up Marketing & Communication  \\\n",
       "1                NaN     NaN                NaN                       NaN   \n",
       "2                NaN     NaN                NaN                       NaN   \n",
       "3                NaN     NaN                NaN                       NaN   \n",
       "\n",
       "  Offer  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interview_id', 'answer_nps'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of reviews\n",
    "train_text_list = df['Review'].tolist()\n",
    "print(train_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_list = []\n",
    "for i in train_text_list:\n",
    "    tagged_text_list = nltk.word_tokenize(i)\n",
    "    pos_tagged = nltk.pos_tag(tagged_text_list)\n",
    "    tagged_list.append(pos_tagged)\n",
    "\n",
    "\n",
    "print(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the word with tag- noun,adjective,verb,adverb, number\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['CD','NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "#train list after filter\n",
    "final_tagged_list=filterTag(tagged_list)\n",
    "print(final_tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Review':final_tagged_list}\n",
    "data = pd.DataFrame(data)\n",
    "data.index = range(1,len(data)+1)\n",
    "df[\"Review\"] = data[\"Review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect Extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1390, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-ff6f2f58cce6>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df.drop('Review',1)\n",
      "<ipython-input-11-ff6f2f58cce6>:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_train = df_sample_train.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "df_sample_train = df_train.sample(frac=1, replace=True, random_state=30)\n",
    "\n",
    "\n",
    "X_train = df_sample_train.Review\n",
    "y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "y_train = y_train.fillna(10)\n",
    "y_train = y_train.astype(str).astype(int)\n",
    "y_train[y_train <= 0] = 1\n",
    "y_train[y_train == 10] = 0\n",
    "\n",
    "y_test = y_test.fillna(10)\n",
    "y_test = y_test.astype(str).astype(int)\n",
    "y_test[y_test <= 0] = 1\n",
    "y_test[y_test == 10] = 0\n",
    "\n",
    "\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "y_train_aspect = y_train.fillna(10)\n",
    "y_train_aspect = y_train_aspect.astype(str).astype(int)\n",
    "y_train_aspect[y_train_aspect <= 0] = 1\n",
    "y_train_aspect[y_train_aspect == 10] = 0\n",
    "\n",
    "y_test_aspect = y_test.fillna(10)\n",
    "y_test_aspect = y_test_aspect.astype(str).astype(int)\n",
    "y_test_aspect[y_test_aspect <= 0] = 1\n",
    "y_test_aspect[y_test_aspect == 10] = 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_train_aspect = np.asarray(y_train_aspect, dtype=np.int64)\n",
    "y_test_aspect = np.asarray(y_test_aspect, dtype=np.int64)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1251, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_aspect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.fillna(10)\n",
    "y = y.astype(str).astype(int)\n",
    "y[y <= 0] = 1\n",
    "y[y == 10] = 0\n",
    "y = np.asarray(y, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-c735abb46c90>:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df3.drop('TITLE', 1)\n"
     ]
    }
   ],
   "source": [
    "y2 = y.tolist()\n",
    "df3 = pd.DataFrame(columns=['target_list'], index=range(5))\n",
    "index = 0\n",
    "for i in y2:\n",
    "    df3.loc[index, 'target_list'] = i\n",
    "    index = index + 1\n",
    "    \n",
    "df3[\"TITLE\"] = data[\"Review\"]\n",
    "df3 = df3[['TITLE', 'target_list']]\n",
    "\n",
    "\n",
    "y = df3.drop('TITLE', 1)\n",
    "X = df3['TITLE']\n",
    "X_train_egal, X_test_ensemble, y_train_egal, y_test_ensemble = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data\n",
    "\n",
    "X = []\n",
    "sentences = list(data[\"Review\"])\n",
    "for sen in sentences:\n",
    "    X.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-a023255a5894>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  Z = df.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "Z = df.drop('Review',1)\n",
    "Z = Z.fillna(10)\n",
    "Z = Z.astype(str).astype(int)\n",
    "Z[Z <= 0] = 1\n",
    "Z[Z == 10] = 0\n",
    "y = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1251, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM, Conv2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import tensorflow.keras.optimizers \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(X_train, vocab_size):\n",
    "# channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 20)           60000     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 20)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 198, 300)          18300     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14)                4214      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 82,514\n",
      "Trainable params: 82,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1251/1251 [==============================] - 9s 5ms/step - loss: 0.3067 - categorical_accuracy: 0.5890 - val_loss: 0.2353 - val_categorical_accuracy: 0.5827\n",
      "Epoch 2/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.1851 - categorical_accuracy: 0.7242 - val_loss: 0.2252 - val_categorical_accuracy: 0.6403\n",
      "Epoch 3/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.1338 - categorical_accuracy: 0.7695 - val_loss: 0.2271 - val_categorical_accuracy: 0.6187\n",
      "Epoch 4/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.1020 - categorical_accuracy: 0.7773 - val_loss: 0.2410 - val_categorical_accuracy: 0.6331\n",
      "Epoch 5/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0778 - categorical_accuracy: 0.7904 - val_loss: 0.2559 - val_categorical_accuracy: 0.6187\n",
      "Epoch 6/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0572 - categorical_accuracy: 0.8221 - val_loss: 0.2670 - val_categorical_accuracy: 0.6259\n",
      "Epoch 7/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0377 - categorical_accuracy: 0.8199 - val_loss: 0.3005 - val_categorical_accuracy: 0.6043\n",
      "Epoch 8/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0243 - categorical_accuracy: 0.7929 - val_loss: 0.3107 - val_categorical_accuracy: 0.5971\n",
      "Epoch 9/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0186 - categorical_accuracy: 0.8080 - val_loss: 0.3726 - val_categorical_accuracy: 0.6187\n",
      "Epoch 10/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0126 - categorical_accuracy: 0.8253 - val_loss: 0.3848 - val_categorical_accuracy: 0.5683\n",
      "Epoch 11/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0109 - categorical_accuracy: 0.7995 - val_loss: 0.4209 - val_categorical_accuracy: 0.5827\n",
      "Epoch 12/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0074 - categorical_accuracy: 0.7896 - val_loss: 0.4420 - val_categorical_accuracy: 0.5899\n",
      "Epoch 13/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0063 - categorical_accuracy: 0.7977 - val_loss: 0.4343 - val_categorical_accuracy: 0.5899\n",
      "Epoch 14/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0041 - categorical_accuracy: 0.7973 - val_loss: 0.4363 - val_categorical_accuracy: 0.6187\n",
      "Epoch 15/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0038 - categorical_accuracy: 0.8213 - val_loss: 0.4376 - val_categorical_accuracy: 0.5755\n",
      "Epoch 16/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0037 - categorical_accuracy: 0.8157 - val_loss: 0.4438 - val_categorical_accuracy: 0.5899\n",
      "Epoch 17/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0032 - categorical_accuracy: 0.8277 - val_loss: 0.4379 - val_categorical_accuracy: 0.5755\n",
      "Epoch 18/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0029 - categorical_accuracy: 0.8055 - val_loss: 0.4514 - val_categorical_accuracy: 0.5899\n",
      "Epoch 19/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0025 - categorical_accuracy: 0.8116 - val_loss: 0.4519 - val_categorical_accuracy: 0.5683\n",
      "Epoch 20/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.8301 - val_loss: 0.4503 - val_categorical_accuracy: 0.5612\n",
      "Epoch 21/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0023 - categorical_accuracy: 0.8297 - val_loss: 0.4609 - val_categorical_accuracy: 0.5971\n",
      "Epoch 22/100\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.0018 - categorical_accuracy: 0.8126 - val_loss: 0.4638 - val_categorical_accuracy: 0.6043\n",
      "Epoch 23/100\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.0019 - categorical_accuracy: 0.8118 - val_loss: 0.4654 - val_categorical_accuracy: 0.6043\n",
      "Epoch 24/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.8266 - val_loss: 0.4662 - val_categorical_accuracy: 0.5971\n",
      "Epoch 25/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0022 - categorical_accuracy: 0.8480 - val_loss: 0.4660 - val_categorical_accuracy: 0.5971\n",
      "Epoch 26/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.8372 - val_loss: 0.4674 - val_categorical_accuracy: 0.5971\n",
      "Epoch 27/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.8278 - val_loss: 0.4663 - val_categorical_accuracy: 0.5971\n",
      "Epoch 28/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0022 - categorical_accuracy: 0.8299 - val_loss: 0.4667 - val_categorical_accuracy: 0.5899\n",
      "Epoch 29/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0018 - categorical_accuracy: 0.8267 - val_loss: 0.4678 - val_categorical_accuracy: 0.5971\n",
      "Epoch 30/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0018 - categorical_accuracy: 0.8386 - val_loss: 0.4672 - val_categorical_accuracy: 0.5827\n",
      "Epoch 31/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0016 - categorical_accuracy: 0.8191 - val_loss: 0.4676 - val_categorical_accuracy: 0.5827\n",
      "Epoch 32/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.8102 - val_loss: 0.4683 - val_categorical_accuracy: 0.5827\n",
      "Epoch 33/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0021 - categorical_accuracy: 0.8122 - val_loss: 0.4683 - val_categorical_accuracy: 0.5827\n",
      "Epoch 34/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0021 - categorical_accuracy: 0.8276 - val_loss: 0.4685 - val_categorical_accuracy: 0.5827\n",
      "Epoch 35/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0023 - categorical_accuracy: 0.8266 - val_loss: 0.4683 - val_categorical_accuracy: 0.5827\n",
      "Epoch 36/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0016 - categorical_accuracy: 0.8163 - val_loss: 0.4684 - val_categorical_accuracy: 0.5827\n",
      "Epoch 37/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0016 - categorical_accuracy: 0.8185 - val_loss: 0.4684 - val_categorical_accuracy: 0.5827\n",
      "Epoch 38/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0015 - categorical_accuracy: 0.7921 - val_loss: 0.4686 - val_categorical_accuracy: 0.5827\n",
      "Epoch 39/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.8163 - val_loss: 0.4686 - val_categorical_accuracy: 0.5827\n",
      "Epoch 40/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0018 - categorical_accuracy: 0.8078 - val_loss: 0.4685 - val_categorical_accuracy: 0.5827\n",
      "Epoch 41/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0017 - categorical_accuracy: 0.8444 - val_loss: 0.4684 - val_categorical_accuracy: 0.5827\n",
      "Epoch 42/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0015 - categorical_accuracy: 0.8289 - val_loss: 0.4684 - val_categorical_accuracy: 0.5827\n",
      "Epoch 43/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.7983 - val_loss: 0.4684 - val_categorical_accuracy: 0.5827\n",
      "Epoch 44/100\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.0015 - categorical_accuracy: 0.8098 - val_loss: 0.4684 - val_categorical_accuracy: 0.5827\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "filter_length = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(3000, 20, input_length=maxlen))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(14))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "adam = tf.optimizers.Adam(lr=0.0001, clipvalue=0.5)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(monitor=\"loss\", mode=\"min\", patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_model = tensorflow.keras.models.load_model('model-conv1d.h5')\n",
    "metrics = model.evaluate(X_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[0], metrics[0]))\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-379ebbefb908>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test_ensemble = X_test_ensemble.drop('index', 1)\n",
      "<ipython-input-30-379ebbefb908>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_test_ensemble = y_test_ensemble.drop('index', 1)\n"
     ]
    }
   ],
   "source": [
    "X_test_ensemble = X_test_ensemble.reset_index() \n",
    "y_test_ensemble = y_test_ensemble.reset_index() \n",
    "X_test_ensemble = X_test_ensemble.drop('index', 1)\n",
    "y_test_ensemble = y_test_ensemble.drop('index', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    target_list\n",
      "0    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1    [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "2    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "..                                          ...\n",
      "134  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "135  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "136  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "137  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "138  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "[139 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.tolist()\n",
    "data2={'CNN_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ensemble['target_list'] = y_test_ensemble\n",
    "X_test_ensemble['CNN_Predictions'] = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ensemble.to_csv('ac/cnn_predictions.csv', index=False)  \n",
    "X_test_ensemble.to_csv('bagging/ac/cnn_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect Sentiment Analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-35e333a7b058>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df.drop('Review',1)\n",
      "<ipython-input-88-35e333a7b058>:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_train = df_sample_train.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "#Create binary datasets for positive, negative and neutral sentiments\n",
    "#Positive Dataset\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=1, replace=True, random_state=30)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "\n",
    "#Train Dataset\n",
    "df_positive_train_X = X_train\n",
    "df_positive_train_y = y_train\n",
    "\n",
    "df_positive_train_y = df_positive_train_y.fillna(0)\n",
    "df_positive_train_y = df_positive_train_y.astype(str).astype(int)\n",
    "df_positive_train_y[df_positive_train_y <= 0] = 0\n",
    "\n",
    "#df_positive_train = df_positive_train_X.to_frame().merge(df_positive_train_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_train_y[\"Review\"] = df_positive_train_X\n",
    "df_positive_train = df_positive_train_y\n",
    "\n",
    "#Test Dataset\n",
    "df_positive_test_X = X_test\n",
    "df_positive_test_y = y_test\n",
    "\n",
    "df_positive_test_y = df_positive_test_y.fillna(0)\n",
    "df_positive_test_y = df_positive_test_y.astype(str).astype(int)\n",
    "df_positive_test_y[df_positive_test_y <= 0] = 0\n",
    "\n",
    "#df_positive_test = df_positive_test_X.to_frame().merge(df_positive_test_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_test_y[\"Review\"] = df_positive_test_X\n",
    "df_positive_test = df_positive_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all aspects in one list\n",
    "all_aspects = []\n",
    "for col in y_train.columns:\n",
    "    all_aspects.append(col)\n",
    "print(all_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "X_train_dtm = vect.fit_transform(X_train.astype(str))\n",
    "X_test_dtm = vect.fit_transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating extra feature that indicates which aspect category is present in the review\n",
    "def get_dict_aspect(y,all_aspects):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(all_aspects)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Creates a dictonionary of all aspects and a binary classifier which indicates if a aspect occour in a sentence.\n",
    "train_dict_aspect=get_dict_aspect(y_train_aspect, all_aspects)\n",
    "\n",
    "#Declare DictVectorizer\n",
    "d_train=DictVectorizer()\n",
    "\n",
    "X_train_aspect_dtm = d_train.fit_transform(train_dict_aspect)\n",
    "\n",
    "\n",
    "#The same for the test dataset\n",
    "test_dict_aspect=get_dict_aspect(y_test_aspect, all_aspects)\n",
    "d_test=DictVectorizer()\n",
    "X_test_aspect_dtm = d_test.fit_transform(test_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to classify a sentiment (Used for positive, negative)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def classify_sentiment(df_train,df_test,X_train_aspect_dtm,X_test_aspect_dtm):\n",
    "    \n",
    "    import numpy as np\n",
    "    X_train = df_train.Review\n",
    "    y_train = df_train.drop('Review',1)\n",
    "    y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "    X_test = df_test.Review\n",
    "    y_test = df_test.drop('Review',1)\n",
    "    y_test = np.asarray(y_test, dtype=np.int64)\n",
    "    \n",
    "    #convert sentences to vectors in order to be able to process them \n",
    "    vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "    X_train_dtm = vect_sen.fit_transform(X_train.astype(str))\n",
    "    X_test_dtm = vect_sen.transform(X_test.astype(str))\n",
    "\n",
    "    #ombining word vector with extra feature. (Adds list of aspecs in sentences)\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm))\n",
    "    X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm))\n",
    "\n",
    "\n",
    "    filter_length = 300\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(3000, 20, input_length=6183))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(14))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    adam = tf.optimizers.Adam(lr=0.001, clipvalue=0.5)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    X_train_dtm = pd.DataFrame.sparse.from_spmatrix(X_train_dtm)\n",
    "    X_test_dtm = pd.DataFrame.sparse.from_spmatrix(X_test_dtm)    \n",
    "\n",
    "    callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(monitor=\"val_categorical_accuracy\", mode=\"min\", patience=15, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X_train_dtm, y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=1,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test_dtm, y_test),\n",
    "                        callbacks=callbacks\n",
    "                       )\n",
    "\n",
    "    predictions2 = model.predict(X_test_dtm)\n",
    "\n",
    "    return (predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-3a676e80f029>:8: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_train = df_train.drop('Review',1)\n",
      "<ipython-input-92-3a676e80f029>:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_test = df_test.drop('Review',1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.2611 - categorical_accuracy: 0.5881 - val_loss: 0.2243 - val_categorical_accuracy: 0.7122\n",
      "Epoch 2/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2143 - categorical_accuracy: 0.6333 - val_loss: 0.2242 - val_categorical_accuracy: 0.7122\n",
      "Epoch 3/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2173 - categorical_accuracy: 0.5389 - val_loss: 0.2260 - val_categorical_accuracy: 0.7122\n",
      "Epoch 4/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2117 - categorical_accuracy: 0.5273 - val_loss: 0.2208 - val_categorical_accuracy: 0.6331\n",
      "Epoch 5/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2160 - categorical_accuracy: 0.5295 - val_loss: 0.2251 - val_categorical_accuracy: 0.7122\n",
      "Epoch 6/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2100 - categorical_accuracy: 0.5408 - val_loss: 0.2293 - val_categorical_accuracy: 0.7122\n",
      "Epoch 7/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2066 - categorical_accuracy: 0.5994 - val_loss: 0.2225 - val_categorical_accuracy: 0.7122\n",
      "Epoch 8/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2080 - categorical_accuracy: 0.6124 - val_loss: 0.2255 - val_categorical_accuracy: 0.6906\n",
      "Epoch 9/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2018 - categorical_accuracy: 0.5610 - val_loss: 0.2257 - val_categorical_accuracy: 0.7122\n",
      "Epoch 10/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2048 - categorical_accuracy: 0.5826 - val_loss: 0.2298 - val_categorical_accuracy: 0.6906\n",
      "Epoch 11/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2078 - categorical_accuracy: 0.5366 - val_loss: 0.2275 - val_categorical_accuracy: 0.7122\n",
      "Epoch 12/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2069 - categorical_accuracy: 0.5425 - val_loss: 0.2373 - val_categorical_accuracy: 0.7122\n",
      "Epoch 13/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2027 - categorical_accuracy: 0.6003 - val_loss: 0.2261 - val_categorical_accuracy: 0.7050\n",
      "Epoch 14/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.1992 - categorical_accuracy: 0.5573 - val_loss: 0.2306 - val_categorical_accuracy: 0.7050\n",
      "Epoch 15/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2070 - categorical_accuracy: 0.5859 - val_loss: 0.2272 - val_categorical_accuracy: 0.7050\n",
      "Epoch 16/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2086 - categorical_accuracy: 0.5400 - val_loss: 0.2283 - val_categorical_accuracy: 0.7050\n",
      "Epoch 17/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.1960 - categorical_accuracy: 0.5617 - val_loss: 0.2280 - val_categorical_accuracy: 0.7050\n",
      "Epoch 18/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.2013 - categorical_accuracy: 0.5656 - val_loss: 0.2283 - val_categorical_accuracy: 0.7050\n",
      "Epoch 19/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.1940 - categorical_accuracy: 0.5656 - val_loss: 0.2282 - val_categorical_accuracy: 0.7050\n",
      "Dauer Programmausführung:\n",
      "140.10048246383667\n"
     ]
    }
   ],
   "source": [
    "#For positive sentiment classifier\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "predictions2=classify_sentiment(df_positive_train,df_positive_test,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    target_list  \\\n",
      "1    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "2    [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]   \n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "5    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "..                                          ...   \n",
      "135  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "136  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "137  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]   \n",
      "138  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "139  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                                       CNN_Predictions  \n",
      "1    [0.485784649848938, 0.295086532831192, 0.06323...  \n",
      "2    [0.33474692702293396, 0.3931049406528473, 0.06...  \n",
      "3    [0.49558115005493164, 0.3324110507965088, 0.06...  \n",
      "4    [0.49558115005493164, 0.3324110507965088, 0.06...  \n",
      "5    [0.49558115005493164, 0.3324110507965088, 0.06...  \n",
      "..                                                 ...  \n",
      "135  [0.485784649848938, 0.295086532831192, 0.06323...  \n",
      "136  [0.49558115005493164, 0.3324110507965088, 0.06...  \n",
      "137  [0.3260745406150818, 0.3525644838809967, 0.060...  \n",
      "138  [0.485784649848938, 0.295086532831192, 0.06323...  \n",
      "139  [0.49558115005493164, 0.3324110507965088, 0.06...  \n",
      "\n",
      "[139 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-77ec89a00bb1>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_test = df_positive_test.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "y_test = df_positive_test.drop('Review',1)\n",
    "\n",
    "y_test = np.asarray(y_test, dtype=np.int64)\n",
    "\n",
    "y2 = y_test.tolist()\n",
    "\n",
    "df4 = pd.DataFrame(columns=['target_list'], index=range(5))\n",
    "\n",
    "index = 0\n",
    "for i in y2:\n",
    "    df4.loc[index, 'target_list'] = i\n",
    "    index = index + 1\n",
    "\n",
    "\n",
    "#predictions2 = predictions2.tolist()\n",
    "data2={'CNN_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)\n",
    "\n",
    "data2.index = range(1,len(data2)+1)\n",
    "\n",
    "df4.index = range(1,len(df4)+1)\n",
    "\n",
    "data2['target_list'] = df4\n",
    "data2 = data2[['target_list', 'CNN_Predictions']]\n",
    "\n",
    "data2.to_csv('asa/cnn_predictions.csv', index=False)  \n",
    "#data2.to_csv('bagging/asa/cnn_predictions.csv', index=False)  \n",
    "print(data2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
