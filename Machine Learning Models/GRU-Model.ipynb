{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "zeitanfang = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Exampledataframe.csv\", sep=';', header=1)\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "df = df.rename(columns={'answer_nps_comment': 'Review'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interview_id', 'answer_nps'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of reviews\n",
    "train_text_list = df['Review'].tolist()\n",
    "print(train_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_list = []\n",
    "for i in train_text_list:\n",
    "    tagged_text_list = nltk.word_tokenize(i)\n",
    "    pos_tagged = nltk.pos_tag(tagged_text_list)\n",
    "    tagged_list.append(pos_tagged)\n",
    "\n",
    "\n",
    "print(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the word with tag- noun,adjective,verb,adverb, number\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['CD','NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "#train list after filter\n",
    "final_tagged_list=filterTag(tagged_list)\n",
    "print(final_tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Review':final_tagged_list}\n",
    "data = pd.DataFrame(data)\n",
    "data.index = range(1,len(data)+1)\n",
    "print(data)\n",
    "df[\"Review\"] = data[\"Review\"]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect Extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-1006e2453262>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df.drop('Review',1)\n",
      "<ipython-input-13-1006e2453262>:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_train = df_sample_train.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=1, replace=True, random_state=40)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "y_train = y_train.fillna(10)\n",
    "y_train = y_train.astype(str).astype(int)\n",
    "y_train[y_train <= 0] = 1\n",
    "y_train[y_train == 10] = 0\n",
    "\n",
    "y_test = y_test.fillna(10)\n",
    "y_test = y_test.astype(str).astype(int)\n",
    "y_test[y_test <= 0] = 1\n",
    "y_test[y_test == 10] = 0\n",
    "\n",
    "\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "y_train_aspect = y_train.fillna(10)\n",
    "y_train_aspect = y_train_aspect.astype(str).astype(int)\n",
    "y_train_aspect[y_train_aspect <= 0] = 1\n",
    "y_train_aspect[y_train_aspect == 10] = 0\n",
    "\n",
    "y_test_aspect = y_test.fillna(10)\n",
    "y_test_aspect = y_test_aspect.astype(str).astype(int)\n",
    "y_test_aspect[y_test_aspect <= 0] = 1\n",
    "y_test_aspect[y_test_aspect == 10] = 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_train_aspect = np.asarray(y_train_aspect, dtype=np.int64)\n",
    "y_test_aspect = np.asarray(y_test_aspect, dtype=np.int64)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data\n",
    "\n",
    "X = []\n",
    "sentences = list(data[\"Review\"])\n",
    "for sen in sentences:\n",
    "    X.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a023255a5894>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  Z = df.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "Z = df.drop('Review',1)\n",
    "Z = Z.fillna(10)\n",
    "Z = Z.astype(str).astype(int)\n",
    "Z[Z <= 0] = 1\n",
    "Z[Z == 10] = 0\n",
    "y = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM, Conv2D, SpatialDropout1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import tensorflow.keras.optimizers \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1251/1251 [==============================] - 24s 16ms/step - loss: 0.3082 - accuracy: 0.5619 - val_loss: 0.2277 - val_accuracy: 0.6403\n",
      "Epoch 2/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.2282 - accuracy: 0.5808 - val_loss: 0.2210 - val_accuracy: 0.6331\n",
      "Epoch 3/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.2108 - accuracy: 0.6318 - val_loss: 0.2016 - val_accuracy: 0.6403\n",
      "Epoch 4/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1962 - accuracy: 0.6508 - val_loss: 0.2035 - val_accuracy: 0.6115\n",
      "Epoch 5/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1941 - accuracy: 0.6431 - val_loss: 0.1967 - val_accuracy: 0.6331\n",
      "Epoch 6/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1841 - accuracy: 0.6417 - val_loss: 0.1931 - val_accuracy: 0.6475\n",
      "Epoch 7/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1782 - accuracy: 0.6342 - val_loss: 0.1909 - val_accuracy: 0.6043\n",
      "Epoch 8/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1659 - accuracy: 0.6539 - val_loss: 0.1908 - val_accuracy: 0.6187\n",
      "Epoch 9/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1651 - accuracy: 0.6437 - val_loss: 0.1926 - val_accuracy: 0.6475\n",
      "Epoch 10/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1668 - accuracy: 0.6456 - val_loss: 0.1820 - val_accuracy: 0.6043\n",
      "Epoch 11/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1546 - accuracy: 0.6752 - val_loss: 0.1858 - val_accuracy: 0.5899\n",
      "Epoch 12/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1579 - accuracy: 0.6667 - val_loss: 0.1948 - val_accuracy: 0.6115\n",
      "Epoch 13/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1620 - accuracy: 0.6369 - val_loss: 0.1953 - val_accuracy: 0.5827\n",
      "Epoch 14/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1529 - accuracy: 0.6573 - val_loss: 0.1872 - val_accuracy: 0.6043\n",
      "Epoch 15/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1550 - accuracy: 0.6457 - val_loss: 0.1894 - val_accuracy: 0.6259\n",
      "Epoch 16/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1505 - accuracy: 0.6559 - val_loss: 0.1892 - val_accuracy: 0.5827\n",
      "Epoch 17/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1442 - accuracy: 0.6549 - val_loss: 0.1922 - val_accuracy: 0.6115\n",
      "Epoch 18/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1473 - accuracy: 0.6493 - val_loss: 0.1971 - val_accuracy: 0.5540\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, LSTM, SimpleRNN, Embedding, Dropout, SpatialDropout1D, Activation, Conv1D,GRU\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(monitor=\"loss\", mode=\"min\", patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "maxlen = 200\n",
    "sequence_input = Input(shape=(maxlen, ))\n",
    "model = Embedding(input_dim =2618, input_length=maxlen, output_dim=100,weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "model = SpatialDropout1D(0.5)(model)\n",
    "model = GRU(25, return_sequences=True,dropout=0.1)(model)\n",
    "model = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(model)\n",
    "avg_pool = GlobalAveragePooling1D()(model)\n",
    "max_pool = GlobalMaxPooling1D()(model)\n",
    "model = concatenate([avg_pool, max_pool]) \n",
    "preds = Dense(14, activation=\"sigmoid\")(model)\n",
    "GRU_Glove_model = Model(sequence_input, preds)\n",
    "GRU_Glove_model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model = GRU_Glove_model.fit(X_train, y_train, batch_size=1, epochs=100, validation_data=(X_test, y_test), callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = GRU_Glove_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = GRU_Glove_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.tolist()\n",
    "data2={'GRU_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       GRU_Predictions\n",
      "0    [0.946509838104248, 0.5664821267127991, 0.2368...\n",
      "1    [0.8784558176994324, 0.049329593777656555, 0.0...\n",
      "2    [0.8220332860946655, 0.3336625099182129, 0.142...\n",
      "3    [0.4872305691242218, 0.20868344604969025, 0.00...\n",
      "4    [0.953498363494873, 0.222971573472023, 0.00533...\n",
      "..                                                 ...\n",
      "134  [0.2277623862028122, 0.020373506471514702, 0.8...\n",
      "135  [0.9516741633415222, 0.06668079644441605, 0.94...\n",
      "136  [0.4619810879230499, 0.7922795414924622, 0.107...\n",
      "137  [0.9393295049667358, 0.19237197935581207, 0.02...\n",
      "138  [0.984899640083313, 0.030612053349614143, 0.00...\n",
      "\n",
      "[139 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('ac/gru_predictions.csv', index=False)  \n",
    "#data2.to_csv('bagging/ac/gru_predictions.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "\n",
    "def simple_model():  \n",
    "    maxlen = 200\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    model = Embedding(input_dim =2618, input_length=maxlen, output_dim=100,weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "    model = SpatialDropout1D(0.5)(model)\n",
    "    model = GRU(25, return_sequences=True,dropout=0.1)(model)\n",
    "    model = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(model)\n",
    "    avg_pool = GlobalAveragePooling1D()(model)\n",
    "    max_pool = GlobalMaxPooling1D()(model)\n",
    "    model = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(1, activation=\"sigmoid\")(model)\n",
    "    GRU_Glove_model = Model(sequence_input, preds)\n",
    "    GRU_Glove_model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])\n",
    "    return GRU_Glove_model\n",
    "\n",
    "\n",
    "ann_estimator = KerasRegressor(build_fn=simple_model, epochs=10, batch_size=25, verbose=1)\n",
    "\n",
    "\n",
    "boosted_ann = OneVsRestClassifier(AdaBoostRegressor(base_estimator= ann_estimator, n_estimators = 15))\n",
    "boosted_ann.fit(X_train, y_train)\n",
    "predictions = boosted_ann.predict(X_test)\n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.tolist()\n",
    "data2={'GRU':predictions2}\n",
    "data2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('boosting/gru_boosting.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect-based Sentiment Analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-3e4fa491b959>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "#Create binary datasets for positive, negative and neutral sentiments\n",
    "#Positive Dataset\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=1, replace=True, random_state=40)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "\n",
    "#Train Dataset\n",
    "df_positive_train_X = X_train\n",
    "df_positive_train_y = y_train\n",
    "\n",
    "\n",
    "df_positive_train_y = df_positive_train_y.fillna(0)\n",
    "df_positive_train_y = df_positive_train_y.astype(str).astype(int)\n",
    "df_positive_train_y[df_positive_train_y <= 0] = 0\n",
    "\n",
    "#df_positive_train = df_positive_train_X.to_frame().merge(df_positive_train_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_train_y[\"Review\"] = df_positive_train_X\n",
    "df_positive_train = df_positive_train_y\n",
    "\n",
    "#Test Dataset\n",
    "df_positive_test_X = X_test\n",
    "df_positive_test_y = y_test\n",
    "\n",
    "df_positive_test_y = df_positive_test_y.fillna(0)\n",
    "df_positive_test_y = df_positive_test_y.astype(str).astype(int)\n",
    "df_positive_test_y[df_positive_test_y <= 0] = 0\n",
    "\n",
    "#df_positive_test = df_positive_test_X.to_frame().merge(df_positive_test_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_test_y[\"Review\"] = df_positive_test_X\n",
    "df_positive_test = df_positive_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all aspects in one list\n",
    "all_aspects = []\n",
    "for col in y.columns:\n",
    "    all_aspects.append(col)\n",
    "print(all_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "X_train_dtm = vect.fit_transform(X_train.astype(str))\n",
    "X_test_dtm = vect.fit_transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating extra feature that indicates which aspect category is present in the review\n",
    "def get_dict_aspect(y,all_aspects):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(all_aspects)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Creates a dictonionary of all aspects and a binary classifier which indicates if a aspect occour in a sentence.\n",
    "#y_train_aspect = np.asarray(y_train_aspect, dtype=np.int64)\n",
    "train_dict_aspect=get_dict_aspect(y_train_aspect, all_aspects)\n",
    "#Declare DictVectorizer\n",
    "d_train=DictVectorizer()\n",
    "X_train_aspect_dtm = d_train.fit_transform(train_dict_aspect)\n",
    "\n",
    "\n",
    "#The same for the test dataset\n",
    "test_dict_aspect=get_dict_aspect(y_test_aspect, all_aspects)\n",
    "d_test=DictVectorizer()\n",
    "X_test_aspect_dtm = d_test.fit_transform(test_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to classify a sentiment (Used for positive, negative)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def classify_sentiment(df_train,df_test,X_train_aspect_dtm,X_test_aspect_dtm):\n",
    "    \n",
    "    import numpy as np\n",
    "    X_train = df_train.Review\n",
    "    y_train = df_train.drop('Review',1)\n",
    "    y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "    X_test = df_test.Review\n",
    "    y_test = df_test.drop('Review',1)\n",
    "    y_test = np.asarray(y_test, dtype=np.int64)\n",
    "    \n",
    "    #convert sentences to vectors in order to be able to process them \n",
    "    vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "    X_train_dtm = vect_sen.fit_transform(X_train)\n",
    "    X_test_dtm = vect_sen.transform(X_test.astype(str))\n",
    "\n",
    "    #ombining word vector with extra feature. (Adds list of aspecs in sentences)\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm))\n",
    "    X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm))\n",
    "\n",
    "    from tensorflow.keras.layers import Dense, Input, LSTM, SimpleRNN, Embedding, Dropout, SpatialDropout1D, Activation, Conv1D,GRU\n",
    "    from tensorflow.keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization, Add, Flatten\n",
    "    from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "\n",
    "    filter_length = 300\n",
    "\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=4)\n",
    "    maxlen = 8628\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    model = Embedding(input_dim =2618, input_length=maxlen, output_dim=100,weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "    model = SpatialDropout1D(0.5)(model)\n",
    "    model = GRU(25, return_sequences=True,dropout=0.1)(model)\n",
    "    model = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(model)\n",
    "    avg_pool = GlobalAveragePooling1D()(model)\n",
    "    max_pool = GlobalMaxPooling1D()(model)\n",
    "    model = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(14, activation=\"sigmoid\")(model)\n",
    "    \n",
    "    GRU_Glove_model = Model(sequence_input, preds)\n",
    "    \n",
    "    GRU_Glove_model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])\n",
    "\n",
    "    X_train_dtm = pd.DataFrame.sparse.from_spmatrix(X_train_dtm)\n",
    "\n",
    "    X_test_dtm = pd.DataFrame.sparse.from_spmatrix(X_test_dtm)    \n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(),\n",
    "        EarlyStopping(patience=6),\n",
    "        ModelCheckpoint(filepath='model-lstm.h5', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    GRU_Glove_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    history = GRU_Glove_model.fit(X_train_dtm, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_dtm, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "    predictions2 = GRU_Glove_model.predict(X_test_dtm)\n",
    "\n",
    "    return (predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For positive sentiment classifier\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "predictions=classify_sentiment(df_positive_train,df_positive_test,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "\n",
    "predictions = predictions.tolist()\n",
    "data2={'lstm_Predictions':predictions}\n",
    "data2 = pd.DataFrame(data2)\n",
    "data2.to_csv('asa/gru_predictions.csv', index=False)  \n",
    "#data2.to_csv('bagging/asa/gru_predictions.csv', index=False)  \n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
