{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "zeitanfang = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Exampledataframe.csv\", sep=';', header=1)\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "df = df.rename(columns={'answer_nps_comment': 'Review'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interview_id', 'answer_nps'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of reviews\n",
    "train_text_list = df['Review'].tolist()\n",
    "print(train_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_list = []\n",
    "for i in train_text_list:\n",
    "    tagged_text_list = nltk.word_tokenize(i)\n",
    "    pos_tagged = nltk.pos_tag(tagged_text_list)\n",
    "    tagged_list.append(pos_tagged)\n",
    "\n",
    "\n",
    "print(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the word with tag- noun,adjective,verb,adverb, number\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['CD','NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "#train list after filter\n",
    "final_tagged_list=filterTag(tagged_list)\n",
    "print(final_tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Review':final_tagged_list}\n",
    "data = pd.DataFrame(data)\n",
    "data.index = range(1,len(data)+1)\n",
    "\n",
    "df[\"Review\"] = data[\"Review\"]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect Extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-dad94a487cfd>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df.drop('Review',1)\n",
      "<ipython-input-15-dad94a487cfd>:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_train = df_sample_train.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=1, replace=True, random_state=70)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "y_train = y_train.fillna(10)\n",
    "y_train = y_train.astype(str).astype(int)\n",
    "y_train[y_train <= 0] = 1\n",
    "y_train[y_train == 10] = 0\n",
    "\n",
    "y_test = y_test.fillna(10)\n",
    "y_test = y_test.astype(str).astype(int)\n",
    "y_test[y_test <= 0] = 1\n",
    "y_test[y_test == 10] = 0\n",
    "\n",
    "\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "y_train_aspect = y_train.fillna(10)\n",
    "y_train_aspect = y_train_aspect.astype(str).astype(int)\n",
    "y_train_aspect[y_train_aspect <= 0] = 1\n",
    "y_train_aspect[y_train_aspect == 10] = 0\n",
    "\n",
    "y_test_aspect = y_test.fillna(10)\n",
    "y_test_aspect = y_test_aspect.astype(str).astype(int)\n",
    "y_test_aspect[y_test_aspect <= 0] = 1\n",
    "y_test_aspect[y_test_aspect == 10] = 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_train_aspect = np.asarray(y_train_aspect, dtype=np.int64)\n",
    "y_test_aspect = np.asarray(y_test_aspect, dtype=np.int64)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM, Conv2D, SpatialDropout1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import tensorflow.keras.optimizers \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, SimpleRNN, Embedding, Dropout, SpatialDropout1D, Activation, Conv1D,GRU\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization, Add, Flatten\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1251/1251 [==============================] - 21s 15ms/step - loss: 0.3541 - categorical_accuracy: 0.5245 - val_loss: 0.2729 - val_categorical_accuracy: 0.6403\n",
      "Epoch 2/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.2304 - categorical_accuracy: 0.6077 - val_loss: 0.2621 - val_categorical_accuracy: 0.6259\n",
      "Epoch 3/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.2066 - categorical_accuracy: 0.6087 - val_loss: 0.2544 - val_categorical_accuracy: 0.6259\n",
      "Epoch 4/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1922 - categorical_accuracy: 0.6592 - val_loss: 0.2466 - val_categorical_accuracy: 0.6115\n",
      "Epoch 5/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1957 - categorical_accuracy: 0.6543 - val_loss: 0.2361 - val_categorical_accuracy: 0.6331\n",
      "Epoch 6/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1938 - categorical_accuracy: 0.6372 - val_loss: 0.2303 - val_categorical_accuracy: 0.5612\n",
      "Epoch 7/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1854 - categorical_accuracy: 0.6643 - val_loss: 0.2382 - val_categorical_accuracy: 0.5683\n",
      "Epoch 8/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1725 - categorical_accuracy: 0.6795 - val_loss: 0.2235 - val_categorical_accuracy: 0.6331\n",
      "Epoch 9/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1718 - categorical_accuracy: 0.6944 - val_loss: 0.2142 - val_categorical_accuracy: 0.6187\n",
      "Epoch 10/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1684 - categorical_accuracy: 0.6749 - val_loss: 0.2213 - val_categorical_accuracy: 0.6115\n",
      "Epoch 11/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1594 - categorical_accuracy: 0.7029 - val_loss: 0.2278 - val_categorical_accuracy: 0.5971\n",
      "Epoch 12/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1573 - categorical_accuracy: 0.6532 - val_loss: 0.2189 - val_categorical_accuracy: 0.5971\n",
      "Epoch 13/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1472 - categorical_accuracy: 0.6884 - val_loss: 0.2094 - val_categorical_accuracy: 0.5971\n",
      "Epoch 14/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1542 - categorical_accuracy: 0.6984 - val_loss: 0.2120 - val_categorical_accuracy: 0.5899\n",
      "Epoch 15/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1528 - categorical_accuracy: 0.6773 - val_loss: 0.2091 - val_categorical_accuracy: 0.5540\n",
      "Epoch 16/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1460 - categorical_accuracy: 0.6920 - val_loss: 0.2077 - val_categorical_accuracy: 0.5683\n",
      "Epoch 17/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1374 - categorical_accuracy: 0.7010 - val_loss: 0.2114 - val_categorical_accuracy: 0.6187\n",
      "Epoch 18/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1346 - categorical_accuracy: 0.7144 - val_loss: 0.2082 - val_categorical_accuracy: 0.5971\n",
      "Epoch 19/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1420 - categorical_accuracy: 0.6998 - val_loss: 0.2133 - val_categorical_accuracy: 0.5827\n",
      "Epoch 20/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1378 - categorical_accuracy: 0.6956 - val_loss: 0.2001 - val_categorical_accuracy: 0.6115\n",
      "Epoch 21/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1419 - categorical_accuracy: 0.6869 - val_loss: 0.2010 - val_categorical_accuracy: 0.5971\n",
      "Epoch 22/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1300 - categorical_accuracy: 0.7145 - val_loss: 0.2022 - val_categorical_accuracy: 0.6259\n",
      "Epoch 23/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1280 - categorical_accuracy: 0.7161 - val_loss: 0.2001 - val_categorical_accuracy: 0.5971\n",
      "Epoch 24/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1386 - categorical_accuracy: 0.6804 - val_loss: 0.1968 - val_categorical_accuracy: 0.6187\n",
      "Epoch 25/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1272 - categorical_accuracy: 0.7408 - val_loss: 0.1893 - val_categorical_accuracy: 0.5971\n",
      "Epoch 26/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1294 - categorical_accuracy: 0.7113 - val_loss: 0.2018 - val_categorical_accuracy: 0.6115\n",
      "Epoch 27/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1224 - categorical_accuracy: 0.7230 - val_loss: 0.1935 - val_categorical_accuracy: 0.5827\n",
      "Epoch 28/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1306 - categorical_accuracy: 0.7118 - val_loss: 0.1953 - val_categorical_accuracy: 0.5971\n",
      "Epoch 29/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1202 - categorical_accuracy: 0.7103 - val_loss: 0.1946 - val_categorical_accuracy: 0.5899\n",
      "Epoch 30/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1180 - categorical_accuracy: 0.6933 - val_loss: 0.1995 - val_categorical_accuracy: 0.5971\n",
      "Epoch 31/100\n",
      "1251/1251 [==============================] - 19s 15ms/step - loss: 0.1208 - categorical_accuracy: 0.7218 - val_loss: 0.1976 - val_categorical_accuracy: 0.6115\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor=\"val_accuracy\", mode=\"min\", patience=4, restore_best_weights=True)\n",
    "\n",
    "Bil_LSTM_Glove_model = Sequential([\n",
    "    Embedding(input_dim =embedding_matrix.shape[0], input_length=maxlen, output_dim=embedding_matrix.shape[1],weights=[embedding_matrix], trainable=False),\n",
    "    SpatialDropout1D(0.5),\n",
    "    LSTM(25, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(50, activation = 'relu'),\n",
    "    Dense(14, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "LSTM_Glove_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['categorical_accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "model = LSTM_Glove_model.fit(X_train, y_train, batch_size=1, epochs=100, validation_data=(X_test, y_test), callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = Bil_LSTM_Glove_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Bil_LSTM_Glove_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.tolist()\n",
    "data2={'LSTM_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      LSTM_Predictions\n",
      "0    [0.7144482135772705, 0.5951213836669922, 0.066...\n",
      "1    [0.8619893789291382, 0.11617100983858109, 0.03...\n",
      "2    [0.5445446372032166, 0.6281680464744568, 0.223...\n",
      "3    [0.2855740487575531, 0.20996013283729553, 0.03...\n",
      "4    [0.9365394115447998, 0.1364806890487671, 0.006...\n",
      "..                                                 ...\n",
      "134  [0.2827754318714142, 0.06759277731180191, 0.19...\n",
      "135  [0.6869401335716248, 0.37167888879776, 0.77627...\n",
      "136  [0.498079389333725, 0.8263464570045471, 0.0365...\n",
      "137  [0.8505749106407166, 0.22587460279464722, 0.02...\n",
      "138  [0.9605934619903564, 0.04700348898768425, 0.01...\n",
      "\n",
      "[139 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('ac/lstm_predictions.csv', index=False)  \n",
    "#data2.to_csv('bagging/ac/lstm_predictions.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect-based Sentiment Analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary datasets for positive, negative and neutral sentiments\n",
    "#Positive Dataset\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=1, replace=True, random_state=70)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "#Train Dataset\n",
    "df_positive_train_X = X_train\n",
    "df_positive_train_y = y_train\n",
    "\n",
    "\n",
    "df_positive_train_y = df_positive_train_y.fillna(0)\n",
    "df_positive_train_y = df_positive_train_y.astype(str).astype(int)\n",
    "df_positive_train_y[df_positive_train_y <= 0] = 0\n",
    "\n",
    "#df_positive_train = df_positive_train_X.to_frame().merge(df_positive_train_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_train_y[\"Review\"] = df_positive_train_X\n",
    "df_positive_train = df_positive_train_y\n",
    "\n",
    "#Test Dataset\n",
    "df_positive_test_X = X_test\n",
    "df_positive_test_y = y_test\n",
    "\n",
    "df_positive_test_y = df_positive_test_y.fillna(0)\n",
    "df_positive_test_y = df_positive_test_y.astype(str).astype(int)\n",
    "df_positive_test_y[df_positive_test_y <= 0] = 0\n",
    "\n",
    "#df_positive_test = df_positive_test_X.to_frame().merge(df_positive_test_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_test_y[\"Review\"] = df_positive_test_X\n",
    "df_positive_test = df_positive_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all aspects in one list\n",
    "all_aspects = []\n",
    "for col in y.columns:\n",
    "    all_aspects.append(col)\n",
    "print(all_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "X_train_dtm = vect.fit_transform(X_train.astype(str))\n",
    "X_test_dtm = vect.fit_transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating extra feature that indicates which aspect category is present in the review\n",
    "def get_dict_aspect(y,all_aspects):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(all_aspects)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Creates a dictonionary of all aspects and a binary classifier which indicates if a aspect occour in a sentence.\n",
    "#y_train_aspect = np.asarray(y_train_aspect, dtype=np.int64)\n",
    "\n",
    "train_dict_aspect=get_dict_aspect(y_train_aspect, all_aspects)\n",
    "#Declare DictVectorizer\n",
    "d_train=DictVectorizer()\n",
    "\n",
    "X_train_aspect_dtm = d_train.fit_transform(train_dict_aspect)\n",
    "\n",
    "\n",
    "#The same for the test dataset\n",
    "test_dict_aspect=get_dict_aspect(y_test_aspect, all_aspects)\n",
    "d_test=DictVectorizer()\n",
    "X_test_aspect_dtm = d_test.fit_transform(test_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to classify a sentiment (Used for positive, negative)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def classify_sentiment(df_train,df_test,X_train_aspect_dtm,X_test_aspect_dtm):\n",
    "    \n",
    "    import numpy as np\n",
    "    X_train = df_train.Review\n",
    "    y_train = df_train.drop('Review',1)\n",
    "    y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "    X_test = df_test.Review\n",
    "    y_test = df_test.drop('Review',1)\n",
    "    y_test = np.asarray(y_test, dtype=np.int64)\n",
    "    \n",
    "    #convert sentences to vectors in order to be able to process them \n",
    "    vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "    X_train_dtm = vect_sen.fit_transform(X_train)\n",
    "    X_test_dtm = vect_sen.transform(X_test.astype(str))\n",
    "\n",
    "    #ombining word vector with extra feature. (Adds list of aspecs in sentences)\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm))\n",
    "    X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm))\n",
    "\n",
    "\n",
    "    filter_length = 300\n",
    "\n",
    "    Bil_LSTM_Glove_model = Sequential([\n",
    "        Embedding(input_dim =2064, input_length=6646, output_dim=100,weights=[embedding_matrix], trainable=False),\n",
    "        SpatialDropout1D(0.5),\n",
    "        LSTM(25, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(50, activation = 'relu'),\n",
    "        Dense(14, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    X_train_dtm = pd.DataFrame.sparse.from_spmatrix(X_train_dtm)\n",
    "\n",
    "    X_test_dtm = pd.DataFrame.sparse.from_spmatrix(X_test_dtm)    \n",
    "\n",
    "    callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(patience=6, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    LSTM_Glove_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    history = LSTM_Glove_model.fit(X_train_dtm, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_dtm, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "    predictions2 = Bil_LSTM_Glove_model.predict(X_test_dtm)\n",
    "\n",
    "    return (predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For positive sentiment classifier\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "predictions=classify_sentiment(df_positive_train,df_positive_test,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "\n",
    "predictions = predictions.tolist()\n",
    "data2={'lstm_Predictions':predictions}\n",
    "data2 = pd.DataFrame(data2)\n",
    "data2.to_csv('asa/lstm_predictions.csv', index=False)  \n",
    "#data2.to_csv('bagging/asa/lstm_predictions.csv', index=False)  \n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
