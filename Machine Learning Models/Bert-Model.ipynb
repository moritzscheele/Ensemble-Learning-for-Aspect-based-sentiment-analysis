{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Tesla P100-PCIE-16GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "zeitanfang = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Exampledataframe.csv\", sep=';', header=1)\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "df = df.rename(columns={'answer_nps_comment': 'Review'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interview_id', 'answer_nps'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of reviews\n",
    "train_text_list = df['Review'].tolist()\n",
    "print(train_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_list = []\n",
    "for i in train_text_list:\n",
    "    tagged_text_list = nltk.word_tokenize(i)\n",
    "    pos_tagged = nltk.pos_tag(tagged_text_list)\n",
    "    tagged_list.append(pos_tagged)\n",
    "\n",
    "\n",
    "print(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the word with tag- noun,adjective,verb,adverb, number\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['CD','NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "#train list after filter\n",
    "final_tagged_list=filterTag(tagged_list)\n",
    "print(final_tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Review':final_tagged_list}\n",
    "data = pd.DataFrame(data)\n",
    "data.index = range(1,len(data)+1)\n",
    "    \n",
    "df[\"Review\"] = data[\"Review\"]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect Extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333817/1065055103.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  y = df.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "y = y.fillna(10)\n",
    "y = y.astype(str).astype(int)\n",
    "y[y <= 0] = 1\n",
    "y[y == 10] = 0\n",
    "\n",
    "y[\"comment_text\"] = X\n",
    "df = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in ./anaconda3/lib/python3.9/site-packages (1.5.10)\n",
      "Requirement already satisfied: PyYAML>=5.1 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: future>=0.17.1 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: setuptools==59.5.0 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.7.3)\n",
      "Requirement already satisfied: packaging>=17.0 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (21.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (3.10.0.2)\n",
      "Requirement already satisfied: torch>=1.7.* in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.11.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (2021.8.1)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.22.3)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in ./anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in ./anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: wheel>=0.26 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.44.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
      "Requirement already satisfied: transformers in ./anaconda3/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: sacremoses in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: seaborn in ./anaconda3/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in ./anaconda3/lib/python3.9/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in ./anaconda3/lib/python3.9/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: pandas>=0.23 in ./anaconda3/lib/python3.9/site-packages (from seaborn) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.15 in ./anaconda3/lib/python3.9/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./anaconda3/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytorch_lightning\n",
    "!pip3 install transformers\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1251, 15), (139, 15))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "#Only for Bagging\n",
    "#train_df = train_df.sample(frac=1, replace=True, random_state=50)\n",
    "\n",
    "\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "#BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-large-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "\n",
    "    self,\n",
    "\n",
    "    data: pd.DataFrame,\n",
    "\n",
    "    tokenizer: BertTokenizer,\n",
    "\n",
    "    max_token_len: int = 128\n",
    "\n",
    "  ):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.data = data\n",
    "\n",
    "    self.max_token_len = max_token_len\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    comment_text = data_row.comment_text\n",
    "\n",
    "    labels = data_row[LABEL_COLUMNS]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "\n",
    "      comment_text,\n",
    "\n",
    "      add_special_tokens=True,\n",
    "\n",
    "      max_length=self.max_token_len,\n",
    "\n",
    "      return_token_type_ids=False,\n",
    "\n",
    "      padding=\"max_length\",\n",
    "\n",
    "      truncation=True,\n",
    "\n",
    "      return_attention_mask=True,\n",
    "\n",
    "      return_tensors='pt',\n",
    "\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "\n",
    "      comment_text=comment_text,\n",
    "\n",
    "      input_ids=encoding[\"input_ids\"].flatten(),\n",
    "\n",
    "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "\n",
    "      labels=torch.FloatTensor(labels)\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TOKEN_COUNT = 512\n",
    "train_dataset = ToxicCommentsDataset(\n",
    "\n",
    "  train_df,\n",
    "\n",
    "  tokenizer,\n",
    "\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    "\n",
    ")\n",
    "\n",
    "sample_item = train_dataset[0]\n",
    "\n",
    "sample_item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512]), torch.Size([32, 512]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "\n",
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=32, num_workers=2)))\n",
    "\n",
    "sample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def __init__(self, train_df, test_df, tokenizer, batch_size=1, max_token_len=128):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.train_df = train_df\n",
    "\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.max_token_len = max_token_len\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "\n",
    "    self.train_dataset = ToxicCommentsDataset(\n",
    "\n",
    "      self.train_df,\n",
    "\n",
    "      self.tokenizer,\n",
    "\n",
    "      self.max_token_len\n",
    "\n",
    "    )\n",
    "\n",
    "    self.test_dataset = ToxicCommentsDataset(\n",
    "\n",
    "      self.test_df,\n",
    "\n",
    "      self.tokenizer,\n",
    "\n",
    "      self.max_token_len\n",
    "\n",
    "    )\n",
    "\n",
    "  def train_dataloader(self):\n",
    "\n",
    "    return DataLoader(\n",
    "\n",
    "      self.train_dataset,\n",
    "\n",
    "      batch_size=self.batch_size,\n",
    "\n",
    "      shuffle=True,\n",
    "\n",
    "      num_workers=32\n",
    "\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "\n",
    "    return DataLoader(\n",
    "\n",
    "      self.test_dataset,\n",
    "\n",
    "      batch_size= 1,\n",
    "\n",
    "      num_workers=32\n",
    "\n",
    "    )\n",
    "\n",
    "  def test_dataloader(self):\n",
    "\n",
    "    return DataLoader(\n",
    "\n",
    "      self.test_dataset,\n",
    "\n",
    "      batch_size= 1,\n",
    "\n",
    "      num_workers=32\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "data_module = ToxicCommentDataModule(\n",
    "\n",
    "  train_df,\n",
    "\n",
    "  val_df,\n",
    "\n",
    "  tokenizer,\n",
    "\n",
    "  batch_size=BATCH_SIZE,\n",
    "\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentTagger(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    self.n_training_steps = n_training_steps\n",
    "\n",
    "    self.n_warmup_steps = n_warmup_steps\n",
    "\n",
    "    self.criterion = nn.BCELoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    output = self.classifier(output.pooler_output)\n",
    "\n",
    "    output = torch.sigmoid(output)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    if labels is not None:\n",
    "\n",
    "        loss = self.criterion(output, labels)\n",
    "\n",
    "    return loss, output\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_epoch_end(self, outputs):\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for output in outputs:\n",
    "\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "\n",
    "        labels.append(out_labels)\n",
    "\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    for i, name in enumerate(LABEL_COLUMNS):\n",
    "\n",
    "      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "\n",
    "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "\n",
    "    optimizer = AdamW(self.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "\n",
    "      optimizer,\n",
    "\n",
    "      num_warmup_steps=self.n_warmup_steps,\n",
    "\n",
    "      num_training_steps=self.n_training_steps\n",
    "\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "\n",
    "      optimizer=optimizer,\n",
    "\n",
    "      lr_scheduler=dict(\n",
    "\n",
    "        scheduler=scheduler,\n",
    "\n",
    "        interval='step'\n",
    "\n",
    "      )\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10008, 50040)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ToxicCommentTagger(\n",
    "\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "\n",
    "  n_warmup_steps=warmup_steps,\n",
    "\n",
    "  n_training_steps=total_training_steps\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "\n",
    "  dirpath=\"checkpoints\",\n",
    "\n",
    "  filename=\"best-checkpoint\",\n",
    "\n",
    "  save_top_k=1,\n",
    "\n",
    "  verbose=True,\n",
    "\n",
    "  monitor=\"val_loss\",\n",
    "\n",
    "  mode=\"min\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"toxic-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f9bc1eabdf0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f9bc1eabdf0>)`.\n",
      "  rank_zero_deprecation(\n",
      "/home/moritz/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "\n",
    "  logger=logger,\n",
    "\n",
    "  checkpoint_callback=checkpoint_callback,\n",
    "\n",
    "  callbacks=[early_stopping_callback],\n",
    "\n",
    "  max_epochs=N_EPOCHS,\n",
    "\n",
    "  gpus=1,\n",
    "\n",
    "  progress_bar_refresh_rate=30\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = ToxicCommentTagger.load_from_checkpoint(\n",
    "\n",
    "  trainer.checkpoint_callback.best_model_path,\n",
    "\n",
    "  n_classes=len(LABEL_COLUMNS)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485bb48f839c4837a249c8056128d230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = ToxicCommentsDataset(\n",
    "\n",
    "  val_df,\n",
    "\n",
    "  tokenizer,\n",
    "\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    "\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "    \n",
    "  _, prediction = trained_model(\n",
    "\n",
    "    item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "\n",
    "    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "\n",
    "  )\n",
    "\n",
    "  predictions.append(prediction.detach().cpu().flatten())\n",
    "\n",
    "  labels.append(item[\"labels\"].detach().cpu().int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.3341e-01, 9.7961e-01, 7.9026e-03,  ..., 1.0088e-03, 1.2697e-04,\n",
      "         8.8558e-05],\n",
      "        [9.8927e-01, 1.0888e-02, 6.0330e-03,  ..., 3.4633e-01, 4.0538e-04,\n",
      "         4.9690e-04],\n",
      "        [4.5959e-02, 9.9311e-01, 6.5355e-03,  ..., 1.8537e-03, 9.0896e-05,\n",
      "         9.9260e-05],\n",
      "        ...,\n",
      "        [3.7834e-01, 9.6892e-01, 2.4645e-02,  ..., 6.4104e-03, 1.4218e-03,\n",
      "         1.5766e-03],\n",
      "        [9.8042e-01, 3.1360e-04, 9.0004e-04,  ..., 2.9400e-02, 6.4657e-05,\n",
      "         7.7782e-05],\n",
      "        [9.9287e-01, 1.6291e-03, 8.1279e-04,  ..., 1.0137e-02, 1.6302e-05,\n",
      "         1.9549e-05]]) tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 0,  ..., 1, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "accuracy(predictions, labels, threshold=THRESHOLD)\n",
    "print(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.tolist()\n",
    "data2={'BERT_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERT_Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.9334074258804321, 0.9796074628829956, 0.007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.9892672300338745, 0.010887877084314823, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.04595877602696419, 0.9931136965751648, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.02699887938797474, 0.9936147928237915, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.6075873970985413, 0.9719584584236145, 0.004...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    BERT_Predictions\n",
       "0  [0.9334074258804321, 0.9796074628829956, 0.007...\n",
       "1  [0.9892672300338745, 0.010887877084314823, 0.0...\n",
       "2  [0.04595877602696419, 0.9931136965751648, 0.00...\n",
       "3  [0.02699887938797474, 0.9936147928237915, 0.00...\n",
       "4  [0.6075873970985413, 0.9719584584236145, 0.004..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('ac/bert_predictions.csv', index=False)  \n",
    "#data2.to_csv('bagging/ac/bert_predictions.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect-based Sentiment Analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2776414/2761705871.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  y = df.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "#Create binary datasets for positive, negative and neutral sentiments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Positive Dataset\n",
    "#Train Dataset\n",
    "df_positive_train_X = X_train\n",
    "df_positive_train_y = y_train\n",
    "\n",
    "df_positive_train_y = df_positive_train_y.fillna(0)\n",
    "df_positive_train_y = df_positive_train_y.astype(str).astype(int)\n",
    "df_positive_train_y[df_positive_train_y <= 0] = 0\n",
    "\n",
    "df_positive_train = df_positive_train_X.to_frame().merge(df_positive_train_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_train = df_positive_train.sample(frac=2, replace=True, random_state=50)\n",
    "\n",
    "\n",
    "#Test Dataset\n",
    "df_positive_test_X = X_test\n",
    "df_positive_test_y = y_test\n",
    "\n",
    "df_positive_test_y = df_positive_test_y.fillna(0)\n",
    "df_positive_test_y = df_positive_test_y.astype(str).astype(int)\n",
    "df_positive_test_y[df_positive_test_y <= 0] = 0\n",
    "\n",
    "df_positive_test = df_positive_test_X.to_frame().merge(df_positive_test_y, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive_train.rename(columns={\"Review\": \"comment_text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = df_positive_train.columns.tolist()[1:15]\n",
    "print(LABEL_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2502, 15)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "#BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-large-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def __init__(self, train_df, test_df, tokenizer, batch_size=1, max_token_len=128):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.train_df = df_positive_train\n",
    "\n",
    "    self.test_df = df_positive_test\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.max_token_len = max_token_len\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "\n",
    "    self.train_dataset = ToxicCommentsDataset(\n",
    "\n",
    "      self.train_df,\n",
    "\n",
    "      self.tokenizer,\n",
    "\n",
    "      self.max_token_len\n",
    "\n",
    "    )\n",
    "\n",
    "    self.test_dataset = ToxicCommentsDataset(\n",
    "\n",
    "      self.test_df,\n",
    "\n",
    "      self.tokenizer,\n",
    "\n",
    "      self.max_token_len\n",
    "\n",
    "    )\n",
    "\n",
    "  def train_dataloader(self):\n",
    "\n",
    "    return DataLoader(\n",
    "\n",
    "      self.train_dataset,\n",
    "\n",
    "      batch_size=self.batch_size,\n",
    "\n",
    "      shuffle=True,\n",
    "\n",
    "      num_workers=32\n",
    "\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "\n",
    "    return DataLoader(\n",
    "\n",
    "      self.test_dataset,\n",
    "\n",
    "      batch_size= 1,\n",
    "\n",
    "      num_workers=32\n",
    "\n",
    "    )\n",
    "\n",
    "  def test_dataloader(self):\n",
    "\n",
    "    return DataLoader(\n",
    "\n",
    "      self.test_dataset,\n",
    "\n",
    "      batch_size= 1,\n",
    "\n",
    "      num_workers=32\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "MAX_TOKEN_COUNT = 512\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "data_module = ToxicCommentDataModule(\n",
    "\n",
    "  df_positive_train,\n",
    "\n",
    "  df_positive_test,\n",
    "\n",
    "  tokenizer,\n",
    "\n",
    "  batch_size=BATCH_SIZE,\n",
    "\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentTagger(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    self.n_training_steps = n_training_steps\n",
    "\n",
    "    self.n_warmup_steps = n_warmup_steps\n",
    "\n",
    "    self.criterion = nn.BCELoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    output = self.classifier(output.pooler_output)\n",
    "\n",
    "    output = torch.sigmoid(output)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    if labels is not None:\n",
    "\n",
    "        loss = self.criterion(output, labels)\n",
    "\n",
    "    return loss, output\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_epoch_end(self, outputs):\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for output in outputs:\n",
    "\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "\n",
    "        labels.append(out_labels)\n",
    "\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    for i, name in enumerate(LABEL_COLUMNS):\n",
    "\n",
    "      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "\n",
    "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "\n",
    "    optimizer = AdamW(self.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "\n",
    "      optimizer,\n",
    "\n",
    "      num_warmup_steps=self.n_warmup_steps,\n",
    "\n",
    "      num_training_steps=self.n_training_steps\n",
    "\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "\n",
    "      optimizer=optimizer,\n",
    "\n",
    "      lr_scheduler=dict(\n",
    "\n",
    "        scheduler=scheduler,\n",
    "\n",
    "        interval='step'\n",
    "\n",
    "      )\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25020, 125100)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch=len(df_positive_train) // BATCH_SIZE\n",
    "\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "\n",
    "warmup_steps = total_training_steps // 5\n",
    "\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "\n",
    "    self,\n",
    "\n",
    "    data: pd.DataFrame,\n",
    "\n",
    "    tokenizer: BertTokenizer,\n",
    "\n",
    "    max_token_len: int = 128\n",
    "\n",
    "  ):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.data = data\n",
    "\n",
    "    self.max_token_len = max_token_len\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    comment_text = data_row.Review\n",
    "\n",
    "    labels = data_row[LABEL_COLUMNS]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "\n",
    "      comment_text,\n",
    "\n",
    "      add_special_tokens=True,\n",
    "\n",
    "      max_length=self.max_token_len,\n",
    "\n",
    "      return_token_type_ids=False,\n",
    "\n",
    "      padding=\"max_length\",\n",
    "\n",
    "      truncation=True,\n",
    "\n",
    "      return_attention_mask=True,\n",
    "\n",
    "      return_tensors='pt',\n",
    "\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "\n",
    "      comment_text=comment_text,\n",
    "\n",
    "      input_ids=encoding[\"input_ids\"].flatten(),\n",
    "\n",
    "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "\n",
    "      labels=torch.FloatTensor(labels)\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TOKEN_COUNT = 512\n",
    "train_dataset = ToxicCommentsDataset(\n",
    "\n",
    "  df_positive_train,\n",
    "\n",
    "  tokenizer,\n",
    "\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    "\n",
    ")\n",
    "\n",
    "sample_item = train_dataset[0]\n",
    "\n",
    "sample_item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512]), torch.Size([32, 512]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "\n",
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=32, num_workers=2)))\n",
    "\n",
    "sample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ToxicCommentTagger(\n",
    "\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "\n",
    "  n_warmup_steps=warmup_steps,\n",
    "\n",
    "  n_training_steps=total_training_steps\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f0a636b1760>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f0a636b1760>)`.\n",
      "  rank_zero_deprecation(\n",
      "/home/moritz/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"toxic-comments\")\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=6)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "\n",
    "  dirpath=\"checkpoints\",\n",
    "\n",
    "  filename=\"best-checkpoint\",\n",
    "\n",
    "  save_top_k=1,\n",
    "\n",
    "  verbose=True,\n",
    "\n",
    "  monitor=\"val_loss\",\n",
    "\n",
    "  mode=\"min\"\n",
    "\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "\n",
    "  logger=logger,\n",
    "\n",
    "  checkpoint_callback=checkpoint_callback,\n",
    "\n",
    "  callbacks=[early_stopping_callback],\n",
    "\n",
    "  max_epochs=40,\n",
    "\n",
    "  gpus=1,\n",
    "\n",
    "  progress_bar_refresh_rate=30\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ToxicCommentDataModule(\n",
    "\n",
    "      df_positive_train,\n",
    "\n",
    "      df_positive_test,\n",
    "\n",
    "      tokenizer,\n",
    "\n",
    "      batch_size=1,\n",
    "\n",
    "      max_token_len=MAX_TOKEN_COUNT\n",
    "    )\n",
    "\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5182715532948789430fbac365dc1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = ToxicCommentTagger.load_from_checkpoint(\n",
    "\n",
    "  trainer.checkpoint_callback.best_model_path,\n",
    "\n",
    "  n_classes=len(LABEL_COLUMNS)\n",
    "\n",
    ")\n",
    "\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = ToxicCommentsDataset(\n",
    "\n",
    "  df_positive_test,\n",
    "\n",
    "  tokenizer,\n",
    "\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    "\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "    \n",
    "  _, prediction = trained_model(\n",
    "\n",
    "    item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "\n",
    "    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "\n",
    "  )\n",
    "\n",
    "  predictions.append(prediction.detach().cpu().flatten())\n",
    "\n",
    "  labels.append(item[\"labels\"].detach().cpu().int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.tolist()\n",
    "data2={'BERT_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)\n",
    "data2.to_csv('asa/bert_predictions_pos5.csv', index=False)  \n",
    "#data2.to_csv('bagging/asa/bert_predictions_pos.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      BERT_Predictions\n",
      "0    [0.7103497385978699, 0.965736985206604, 0.9856...\n",
      "1    [0.9891267418861389, 0.001976946135982871, 0.0...\n",
      "2    [0.001001407508738339, 0.646804690361023, 0.00...\n",
      "3    [0.002303165150806308, 0.9895614981651306, 0.0...\n",
      "4    [0.9694634079933167, 0.027138778939843178, 0.0...\n",
      "..                                                 ...\n",
      "134  [0.0010807501384988427, 0.008470175787806511, ...\n",
      "135  [0.9980494976043701, 0.0004960295627824962, 0....\n",
      "136  [0.733977198600769, 0.9780758023262024, 0.0009...\n",
      "137  [0.9986697435379028, 0.0009717561770230532, 0....\n",
      "138  [0.9987063407897949, 0.000628592271823436, 0.0...\n",
      "\n",
      "[139 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
