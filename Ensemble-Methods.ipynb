{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"Input/asa_predictions_combined.csv\", sep=';', header=0)\n",
    "df2 = pd.read_csv(\"Input/ac_predictions_combined.csv\", sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvoting = df.copy()\n",
    "for ind, i in enumerate(hvoting.CNN_Predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 0.5:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0\n",
    "\n",
    "for ind, i in enumerate(hvoting.BERT_Predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 0.5:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0\n",
    "        \n",
    "for ind, i in enumerate(hvoting.GRU_Predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 0.5:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0\n",
    "\n",
    "for ind, i in enumerate(hvoting.MLP_Predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 0.5:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0\n",
    "\n",
    "for ind, i in enumerate(hvoting.LSTM_Predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 0.5:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "hvoting[\"Ensemble_predictions\"] = \"\"\n",
    "hvoting['Ensemble_predictions'] = hvoting['Ensemble_predictions'].astype(object)\n",
    "\n",
    "\n",
    "for ind in hvoting.index:\n",
    "    x = list( map(add, hvoting['CNN_Predictions'][ind], hvoting['BERT_Predictions'][ind] ))\n",
    "    x2 = list( map(add, x, hvoting['GRU_Predictions'][ind] ))\n",
    "    x3 = list( map(add, x2, hvoting['MLP_Predictions'][ind] ))\n",
    "    x4 = list( map(add, x3, hvoting['LSTM_Predictions'][ind] ))\n",
    "    hvoting.at[ind, 'Ensemble_predictions'] = x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, i in enumerate(hvoting.Ensemble_predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 3:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = hvoting.target_list.tolist()\n",
    "y_test = np.asarray(y_test, dtype=np.int64)\n",
    "y_pred = hvoting.Ensemble_predictions.tolist()\n",
    "\n",
    "#Metrics \n",
    "import sklearn.metrics\n",
    "\n",
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if (sum(np.logical_and(y_true[i], y_pred[i])) != 0) & (sum(np.logical_or(y_true[i], y_pred[i])) != 0):\n",
    "            temp += (sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i])))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "def Loss_Metric(y_true, y_pred):\n",
    "    return np.any(y_true != y_pred, axis=1).mean()\n",
    "    \n",
    "#MR = np.all(y_pred_class_rfc == y_test, axis=1).mean()\n",
    "#Loss_Metric = np.any(y_test != y_pred_class_rfc, axis=1).mean()\n",
    "\n",
    "print('Exact Match Ratio: {0}'.format(sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)))\n",
    "\n",
    "print('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(y_test, y_pred))) \n",
    "\n",
    "print('Accuracy: {0}'.format(Accuracy(y_test, y_pred)))\n",
    "\n",
    "print('Recall: {0}'.format(sklearn.metrics.precision_score(y_test, y_pred, average='samples'))) \n",
    "\n",
    "print('Precision: {0}'.format(sklearn.metrics.recall_score(y_test, y_pred, average='samples')))\n",
    "\n",
    "print('F1 Measure: {0}'.format(sklearn.metrics.f1_score(y_test, y_pred, average='samples'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svoting = df.copy()\n",
    "\n",
    "for index, i in enumerate(svoting.CNN_Predictions):\n",
    "        svoting.CNN_Predictions[index] = [j * 0 for j in i]\n",
    "        \n",
    "for index, i in enumerate(svoting.MLP_Predictions):\n",
    "        svoting.MLP_Predictions[index] = [j * 70 for j in i]\n",
    "        \n",
    "for index, i in enumerate(svoting.BERT_Predictions):\n",
    "        svoting.BERT_Predictions[index] = [j * 4 for j in i]        \n",
    "        \n",
    "for index, i in enumerate(svoting.GRU_Predictions):\n",
    "        svoting.GRU_Predictions[index] = [j * 20 for j in i]\n",
    "        \n",
    "for index, i in enumerate(svoting.LSTM_Predictions):\n",
    "        svoting.LSTM_Predictions[index] = [j * 5 for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "svoting[\"Ensemble_predictions\"] = \"\"\n",
    "svoting['Ensemble_predictions'] = svoting['Ensemble_predictions'].astype(object)\n",
    "\n",
    "\n",
    "for ind in svoting.index:\n",
    "    x = list( map(add, svoting['CNN_Predictions'][ind], svoting['MLP_Predictions'][ind] ))\n",
    "    x2 = list( map(add, x, svoting['BERT_Predictions'][ind] ))\n",
    "    x3 = list( map(add, x2, svoting['GRU_Predictions'][ind] ))\n",
    "    x4 = list( map(add, x3, svoting['LSTM_Predictions'][ind] ))\n",
    "    svoting.at[ind, 'Ensemble_predictions'] = x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all values >= 1 to 1 to make the list binary for calculating the accuracy\n",
    "for ind, i in enumerate(svoting.Ensemble_predictions):\n",
    "    for j, n in enumerate(i):\n",
    "        if n >= 26:\n",
    "            i[j] = 1\n",
    "        else:\n",
    "            i[j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = svoting.target_list.tolist()\n",
    "y_test = np.asarray(y_test, dtype=np.int64)\n",
    "y_pred = svoting.Ensemble_predictions.tolist()\n",
    "\n",
    "\n",
    "#Metrics \n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print('Exact Match Ratio: {0}'.format(sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)))\n",
    "\n",
    "print('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(y_test, y_pred))) \n",
    "\n",
    "print('Accuracy: {0}'.format(Accuracy(y_test, y_pred)))\n",
    "\n",
    "print('Recall: {0}'.format(sklearn.metrics.precision_score(y_test, y_pred, average='samples'))) \n",
    "\n",
    "print('Precision: {0}'.format(sklearn.metrics.recall_score(y_test, y_pred, average='samples')))\n",
    "\n",
    "print('F1 Measure: {0}'.format(sklearn.metrics.f1_score(y_test, y_pred, average='samples'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking = df.copy()\n",
    "stacking.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, j in enumerate(stacking.index):\n",
    "    for index, i in enumerate(stacking.CNN_Predictions[j]):\n",
    "        x = \"CNN\" +str(index)\n",
    "        stacking.at[ind, x] = i\n",
    "        \n",
    "    for index, i in enumerate(stacking.RF_Predictions[j]):\n",
    "        x = \"RF\" +str(index)\n",
    "        stacking.at[ind, x] = i\n",
    "        \n",
    "    for index, i in enumerate(stacking.BERT_Predictions[j]):\n",
    "        x = \"BERT\" +str(index)\n",
    "        stacking.at[ind, x] = i\n",
    "        \n",
    "    for index, i in enumerate(stacking.GRU_Predictions[j]):\n",
    "        x = \"GRU\" +str(index)\n",
    "        stacking.at[ind, x] = i\n",
    "        \n",
    "    for index, i in enumerate(stacking.LSTM_Predictions[j]):\n",
    "        x = \"LSTM\" +str(index)\n",
    "        stacking.at[ind, x] = i\n",
    "        \n",
    "    for index, i in enumerate(stacking.target_list[j]):\n",
    "        x = \"Y\" +str(index)\n",
    "        stacking.at[ind, x] = i\n",
    "\n",
    "stacking = stacking.drop(['CNN_Predictions', 'RF_Predictions', 'BERT_Predictions','GRU_Predictions', 'LSTM_Predictions',  'target_list', 'TITLE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stacking.drop(['Y0', 'Y1', 'Y2', 'Y3', 'Y4', 'Y5', 'Y6', 'Y7', 'Y8', 'Y9', 'Y10', 'Y11', 'Y12', 'Y13'], axis=1)\n",
    "y = stacking[['Y0', 'Y1', 'Y2', 'Y3', 'Y4', 'Y5', 'Y6', 'Y7', 'Y8', 'Y9', 'Y10', 'Y11', 'Y12', 'Y13']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train, dtype=np.int64)\n",
    "y_test = np.asarray(y_test, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "#Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 300, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausf√ºhrung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_rfc = rf_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.tolist()\n",
    "y_test = np.asarray(y_test, dtype=np.int64)\n",
    "y_pred = y_pred_class_rfc.tolist()\n",
    "\n",
    "#Metrics \n",
    "import sklearn.metrics\n",
    "\n",
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if (sum(np.logical_and(y_true[i], y_pred[i])) != 0) & (sum(np.logical_or(y_true[i], y_pred[i])) != 0):\n",
    "            temp += (sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i])))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "def Loss_Metric(y_true, y_pred):\n",
    "    return np.any(y_true != y_pred, axis=1).mean()\n",
    "\n",
    "\n",
    "print('Exact Match Ratio: {0}'.format(sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)))\n",
    "\n",
    "print('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(y_test, y_pred))) \n",
    "\n",
    "print('Accuracy: {0}'.format(Accuracy(y_test, y_pred)))\n",
    "\n",
    "print('Recall: {0}'.format(sklearn.metrics.precision_score(y_test, y_pred, average='samples'))) \n",
    "\n",
    "print('Precision: {0}'.format(sklearn.metrics.recall_score(y_test, y_pred, average='samples')))\n",
    "\n",
    "print('F1 Measure: {0}'.format(sklearn.metrics.f1_score(y_test, y_pred, average='samples'))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
