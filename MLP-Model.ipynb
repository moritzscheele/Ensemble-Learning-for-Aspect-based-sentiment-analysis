{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "zeitanfang = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Exampledataframe.csv\", sep=';', header=1)\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "df = df.rename(columns={'answer_nps_comment': 'Review'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interview_id', 'answer_nps'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of reviews\n",
    "train_text_list = df['Review'].tolist()\n",
    "print(train_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_list = []\n",
    "for i in train_text_list:\n",
    "    tagged_text_list = nltk.word_tokenize(i)\n",
    "    pos_tagged = nltk.pos_tag(tagged_text_list)\n",
    "    tagged_list.append(pos_tagged)\n",
    "\n",
    "\n",
    "print(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the word with tag- noun,adjective,verb,adverb, number\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['CD','NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list\n",
    "\n",
    "#train list after filter\n",
    "final_tagged_list=filterTag(tagged_list)\n",
    "print(final_tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Review':final_tagged_list}\n",
    "data = pd.DataFrame(data)\n",
    "data.index = range(1,len(data)+1)\n",
    "    \n",
    "df[\"Review\"] = data[\"Review\"]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect Extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-5bfe9399528a>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y = df.drop('Review',1)\n",
      "<ipython-input-11-5bfe9399528a>:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  y_train = df_sample_train.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=1, replace=True, random_state=10)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "y_train_aspect = y_train.fillna(10)\n",
    "y_train_aspect = y_train_aspect.astype(str).astype(int)\n",
    "y_train_aspect[y_train_aspect <= 0] = 1\n",
    "y_train_aspect[y_train_aspect == 10] = 0\n",
    "\n",
    "y_test_aspect = y_test.fillna(10)\n",
    "y_test_aspect = y_test_aspect.astype(str).astype(int)\n",
    "y_test_aspect[y_test_aspect <= 0] = 1\n",
    "y_test_aspect[y_test_aspect == 10] = 0\n",
    "\n",
    "import numpy as np\n",
    "y_train_aspect = np.asarray(y_train_aspect, dtype=np.int64)\n",
    "y_test_aspect = np.asarray(y_test_aspect, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "X_train_dtm = pd.DataFrame.sparse.from_spmatrix(X_train_dtm)\n",
    "    \n",
    "X_test_dtm = pd.DataFrame.sparse.from_spmatrix(X_test_dtm)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1251, 14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_aspect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/envs/tf_gpu2/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "mlp = MLPClassifier().fit(X_train_dtm, y_train_aspect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dauer Programmausführung:\n",
      "191.43975734710693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/envs/tf_gpu2/lib/python3.9/site-packages/sklearn/multiclass.py:79: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "clf = OneVsRestClassifier(AdaBoostClassifier(base_estimator=RandomForestClassifier(), n_estimators=100, random_state=0))\n",
    "clf.fit(X_train_dtm, y_train_aspect)\n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the test data using classifiers\n",
    "#y_pred_class_rfc = rfc.predict_proba(X_test_dtm)\n",
    "\n",
    "y_pred_class_mlp = mlp.predict_proba(X_test_dtm)\n",
    "#y_pred_class_boosting = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = y_pred_class_mlp.tolist()\n",
    "data2={'MLP_Predictions':predictions2}\n",
    "data2 = pd.DataFrame(data2)\n",
    "\n",
    "data2.to_csv('ac/mlp.csv', index=False)  \n",
    "#data2.to_csv('bagging/ac/mlp.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Aspect-based Sentiment Analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333814/1330922059.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  y = df.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "#Create binary datasets for positive, negative and neutral sentiments\n",
    "#Positive Dataset\n",
    "X= df.Review\n",
    "y = df.drop('Review',1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#Only for Bagging\n",
    "#df_train = X_train.to_frame().merge(y_train, how='inner', left_index=True, right_index=True)\n",
    "#df_sample_train = df_train.sample(frac=2, replace=True, random_state=10)\n",
    "\n",
    "\n",
    "#X_train = df_sample_train.Review\n",
    "#y_train = df_sample_train.drop('Review',1)\n",
    "\n",
    "#Train Dataset\n",
    "df_positive_train_X = X_train\n",
    "df_positive_train_y = y_train\n",
    "\n",
    "\n",
    "df_positive_train_y = df_positive_train_y.fillna(0)\n",
    "df_positive_train_y = df_positive_train_y.astype(str).astype(int)\n",
    "df_positive_train_y[df_positive_train_y <= 0] = 0\n",
    "\n",
    "#df_positive_train = df_positive_train_X.to_frame().merge(df_positive_train_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_train_y[\"Review\"] = df_positive_train_X\n",
    "df_positive_train = df_positive_train_y\n",
    "\n",
    "#Test Dataset\n",
    "df_positive_test_X = X_test\n",
    "df_positive_test_y = y_test\n",
    "\n",
    "df_positive_test_y = df_positive_test_y.fillna(0)\n",
    "df_positive_test_y = df_positive_test_y.astype(str).astype(int)\n",
    "df_positive_test_y[df_positive_test_y <= 0] = 0\n",
    "\n",
    "#df_positive_test = df_positive_test_X.to_frame().merge(df_positive_test_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_positive_test_y[\"Review\"] = df_positive_test_X\n",
    "df_positive_test = df_positive_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all aspects in one list\n",
    "all_aspects = []\n",
    "for col in y.columns:\n",
    "    all_aspects.append(col)\n",
    "print(all_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating extra feature that indicates which aspect category is present in the review\n",
    "\n",
    "def get_dict_aspect(y,all_aspects):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(all_aspects)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Creates a dictonionary of all aspects and a binary classifier which indicates if a aspect occour in a sentence.\n",
    "train_dict_aspect=get_dict_aspect(y_train_aspect, all_aspects)\n",
    "\n",
    "#Declare DictVectorizer\n",
    "d_train=DictVectorizer()\n",
    "\n",
    "X_train_aspect_dtm = d_train.fit_transform(train_dict_aspect)\n",
    "\n",
    "\n",
    "#The same for the test dataset\n",
    "test_dict_aspect=get_dict_aspect(y_test_aspect, all_aspects)\n",
    "d_test=DictVectorizer()\n",
    "X_test_aspect_dtm = d_test.fit_transform(test_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to classify a sentiment (Used for positive, negative)\n",
    "\n",
    "def classify_sentiment(df_train,df_test,X_train_aspect_dtm,X_test_aspect_dtm):\n",
    "    \n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    X_train = df_train.Review\n",
    "    y_train = df_train.drop('Review',1)\n",
    "    y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "    X_test = df_test.Review\n",
    "    y_test = df_test.drop('Review',1)\n",
    "    y_test = np.asarray(y_test, dtype=np.int64)\n",
    "\n",
    "    #convert sentences to vectors in order to be able to process them \n",
    "    vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "    X_train_dtm = vect_sen.fit_transform(X_train)\n",
    "    X_test_dtm = vect_sen.transform(X_test)\n",
    "    #ombining word vector with extra feature. (Adds list of aspecs in sentences)\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm))\n",
    "    X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm))\n",
    "    \n",
    "    X_train_dtm = pd.DataFrame.sparse.from_spmatrix(X_train_dtm)\n",
    "    \n",
    "    X_test_dtm = pd.DataFrame.sparse.from_spmatrix(X_test_dtm)   \n",
    "    \n",
    "\n",
    "    #Train models - OneVsRestClassifier: One Classifier per class will be trained\n",
    "    C = 1.0 #SVregularization parameter\n",
    "    #clf = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)).fit(X_train_dtm, y_train_aspect)\n",
    "    mlp = OneVsRestClassifier(MLPClassifier()).fit(X_train_dtm,y_train)\n",
    "\n",
    "    predictions = mlp.predict_proba(X_test_dtm)\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#For positive sentiment classifier\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "predictions=classify_sentiment(df_positive_train,df_positive_test,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "\n",
    "predictions = predictions.tolist()\n",
    "data2={'MLP_Predictions':predictions}\n",
    "data2 = pd.DataFrame(data2)\n",
    "data3 = data2\n",
    "data2.to_csv('asa/mlp_predictions_pos.csv', index=False)  \n",
    "#data2.to_csv('bagging/asa/mlp_predictions_pos.csv', index=False)  \n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333814/2673588956.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  y_train = df_positive_train.drop('Review',1)\n",
      "/tmp/ipykernel_2333814/2673588956.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  y_test = df_positive_test.drop('Review',1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = df_positive_train.Review\n",
    "y_train = df_positive_train.drop('Review',1)\n",
    "y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "X_test = df_positive_test.Review\n",
    "y_test = df_positive_test.drop('Review',1)\n",
    "y_test = np.asarray(y_test, dtype=np.int64)\n",
    "\n",
    "#convert sentences to vectors in order to be able to process them \n",
    "vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "X_train_dtm = vect_sen.fit_transform(X_train).todense()\n",
    "X_test_dtm = vect_sen.transform(X_test).todense()\n",
    "#ombining word vector with extra feature. (Adds list of aspecs in sentences)\n",
    "from scipy.sparse import hstack\n",
    "X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm)).toarray()\n",
    "X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm)).toarray()\n",
    "    \n",
    "#X_train_dtm = pd.DataFrame.sparse.from_spmatrix(X_train_dtm)\n",
    "    \n",
    "#X_test_dtm = pd.DataFrame.sparse.from_spmatrix(X_test_dtm)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import time\n",
    "zeitanfang = time.time()\n",
    "\n",
    "class customMLPClassifer(MLPClassifier):\n",
    "    def resample_with_replacement(self, X_train, y_train, sample_weight):\n",
    "\n",
    "        # normalize sample_weights if not already\n",
    "        sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "        X_train_resampled = np.zeros((len(X_train), len(X_train[0])), dtype=np.float32)\n",
    "        y_train_resampled = np.zeros((len(y_train)), dtype=np.int)\n",
    "        for i in range(len(X_train)):\n",
    "            # draw a number from 0 to len(X_train)-1\n",
    "            draw = np.random.choice(np.arange(len(X_train)), p=sample_weight)\n",
    "\n",
    "            # place the X and y at the drawn number into the resampled X and y\n",
    "            X_train_resampled[i] = X_train[draw]\n",
    "            y_train_resampled[i] = y_train[draw]\n",
    "\n",
    "        return X_train_resampled, y_train_resampled\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if sample_weight is not None:\n",
    "            X, y = self.resample_with_replacement(X, y, sample_weight)\n",
    "\n",
    "        return self._fit(X, y, incremental=(self.warm_start and\n",
    "                                            hasattr(self, \"classes_\")))\n",
    "\n",
    "       \n",
    "\n",
    "adabooster = OneVsRestClassifier(AdaBoostClassifier(base_estimator=customMLPClassifer(), n_estimators=25))\n",
    "\n",
    "adabooster.fit(X_train_dtm,y_train_aspect)\n",
    "\n",
    "zeitende = time.time()\n",
    "print(\"Dauer Programmausführung:\",)\n",
    "print(zeitende-zeitanfang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = adabooster.predict(X_test_dtm)\n",
    "\n",
    "predictions = predictions.tolist()\n",
    "data2={'Boosting_Predictions':predictions}\n",
    "data2 = pd.DataFrame(data2)\n",
    "data3 = data2\n",
    "data2.to_csv('boosting/mlp_predictions_boosting.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
